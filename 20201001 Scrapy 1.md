---
date: 2020.10.25
title: Scrapy 1
tag: scrapy, crawling, bot
---

# Scrapy 1

### 설치부터 스파이더 생성까지

scrapy를 설치한다. (python 3.7.4)

```
pip install scrapy
```

프로젝트를 생성한다.

```
scrapy startproject news
```

스파이더를 생성한다.

```
cd news
cd spiders
scrapy genspider newsbot news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001
```

현재 폴더 상황은 아래와 같다.

```
news
├── scrapy.cfg
└── news
    ├── __init__.py
    ├── items.py
    ├── middlewares.py
    ├── pipelines.py
    ├── settings.py
    └── spiders
        ├── __init__.py
        └── newsbot.py
```

### 구성 및 동작

##### 스파이더(newsbot.py)

스파이더는 크롤링할 시작 리퀘스트 객체들을 정의하고, 파싱하여 데이터를 추출하는 방법과, 링크를 따라가는 방법을 정의한다. CLI에서 인자를 넘겨줄 수 있다.

##### 아이템(items.py)

구조적인 데이터를 추출하기 위해 키와 밸류로 이루어진 아이템을 정의한다. 스파이더의 콜백 메소드(parse)에서 만들어진 아이템(스크랩 데이터)은 아이템 파이프라인을 따라 처리된다. 아이템 로더(Item Loader)를 사용하면 좀 더 편리하게 리스폰스로부터 아이템을 만들 수 있다.

##### 아이템 파이프라인(pipelines.py)

아이템을 처리하기 위한 다양한 동작을 정의할 수 있다. 필요없는 아이템 또는 중복된 아이템을 드롭하기, 아이템을 DB에 저장하기, 스크린샷 경로를 아이템에 추가하기 등이 있다. [예제](https://docs.scrapy.org/en/latest/topics/item-pipeline.html) 

활성화하려면 settings에 선언해주어어야 한다.

```
ITEM_PIPELINES = {
    'myproject.pipelines.PricePipeline': 300, // 숫자는 파이프라인 적용 우선순위
    'myproject.pipelines.JsonWriterPipeline': 800,
}
```

##### 링크

스파이더가 파싱하는 과정에서 링크에 대한 리퀘스트 객체를 스케줄에 추가할 수 있다. 예를 들면, 게시판 리스트를 크롤링하면 게시글 링크 목록을 얻을 수 있는데, 이 링크를 따라 게시글을 하나씩 크롤링할 수 있다.

링크를 따라가는 다음과 같은 방법이 있다.

```python
// 기본
next_page = response.css('li.next a::attr(href)').get()
if next_page is not None:
	next_page = response.urljoin(next_page)
	yield scrapy.Request(next_page, callback=self.parse)
	
// 축약1 (response.follow는 Request를 리턴한다)
next_page = response.css('li.next a::attr(href)').get()
if next_page is not None:
	yield response.follow(next_page, callback=self.parse)

// 축약2
for a in response.css('ul.pager a'):
    yield response.follow(a, callback=self.parse)

// 축약3
yield from response.follow_all(css='ul.pager a', callback=self.parse)
```

##### Scrapy의 동작 순서

Scrapy는 스파이더가 만든 리퀘스트 객체를 스케줄한다.  각 리퀘스트 객체가 응답을 받으면, 리스폰스 객체를 만들어 콜백 메소드(parse)에 인자로 넘겨준다. 콜백 메소드는 이를 파싱한다. 파싱한 아이템(스크랩 데이터)은 아이템 파이프라인을 따라 처리된다. 콜백 메소드에서 (링크를 따라가기 위해) 리퀘스트 객체를 yield 할 경우, Scrapy는 이 리퀘스트 객체를 스케줄한다.

### 첫번째 크롤링 봇

settings.py를 수정한다.

```
ROBOTSTXT_OBEY = False
FEED_FORMAT = "csv"
FEED_URI = "news.csv"
FEED_EXPORT_ENCODING = 'utf-8-sig'
DOWNLOAD_DELAY = 0.25    # 기본값은 0 (sec)
CONCURRENT_REQUESTS = 1  # 기본값은 16
HTTPCACHE_ENABLED = False  # 기본값은 False
```

아이템(Items.py) 소스를 수정한다. 여기서 크롤링한 데이터가 저장될 구조를 정의할 것이다.

```
import scrapy

class NewsItem(scrapy.Item):
    title = scrapy.Field()
    author = scrapy.Field()
    preview = scrapy.Field()
    url = scrapy.Field()
```

스파이더(newsbot.py) 소스를 수정한다. 여기서 크롤링 동작을 정의할 것이다.

```python
import scrapy

class NewsbotSpider(scrapy.Spider):
	name = 'newsbot'
	allowed_domains = ['news.naver.com']
	start_urls = ['http://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001']

	def parse(self, response):
		titles = response.xpath('//*[@id="main_content"]/div[2]/ul/li/dl/dt[2]/a/text()').extract()
		authors = response.css('.writing::text').extract()
		previews = response.css('.lede::text').extract()

		for item in zip(titles, authors, previews):
			scraped_info = {
				'title' : item[0].strip(),
				'author' : item[1].strip(),
				'preview' : item[2].strip(),
			}
			yield scraped_info
```

크롤링을 실행하고 생성된 news.csv 파일을 열어본다.

```
scrapy crawl newsbot
```

### 참고

- [공식 튜토리얼](https://docs.scrapy.org/en/latest/intro/tutorial.html)

- [Scrapy 간단 사용법](https://python-world.tistory.com/entry/Simple-Scrapy)
- [Scrapy 구조 알아보기](https://python-world.tistory.com/entry/Scrapy-architecture)
- [[크롤링] 무작정 시작하기 (4) - Selenium + Scrapy](https://heodolf.tistory.com/13?category=897877)
- [[크롤링] 무작정 시작하기 (7) - scrapyd](https://heodolf.tistory.com/28?category=897877)