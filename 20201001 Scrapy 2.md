---
date: 2020.11.11
title: Scrapy 2
tag: scrapy, crawling, bot
---

# Scrapy 2

### DB에 저장하기

pipelines.py 파일을 수정하여 수집한 아이템을 DB 저장 처리한다.

```python
class NewsPipeline:
	def process_item(self, item, spider):
		# DB 저장...
		pass
	
	def open_spider(self, spider):
		db.connect()
	
	def close_spider(self, spider):
		db.close()	
```

settings.py 파일을 수정하여 NewsPipeline을 사용하도록 설정한다.

```python
ITEM_PIPELINES = {
	'news.pipelines.NewsPipeline': 100 # 숫자는 해당 파이프라인의 우선순위
}
```

### 크롤링 반복하기

Scrapy는 커맨드라인에서 실행하는 것이 기본이지만, 스크립트를 통해 실행할 수도 있다. CrawlerProcess의 crawl 메서드가 twisted의 deferred를 리턴하는데, deferred.addCallback을 통해 재귀적으로 crawl 메서드를 수행하면 크롤링 반복이 가능해진다. [참고](http://crawl.blog/scrapy-loop.html)

``` python
def _crawl(result, spider):
    deferred = process.crawl(spider)
    deferred.addCallback(_crawl, spider)
    return deferred
_crawl(None, NewsbotSpider)
```

### 참고

- [공식 튜토리얼](https://docs.scrapy.org/en/latest/intro/tutorial.html)
- [Scrapy 간단 사용법](https://python-world.tistory.com/entry/Simple-Scrapy)
- [Scrapy 구조 알아보기](https://python-world.tistory.com/entry/Scrapy-architecture)
- [[크롤링] 무작정 시작하기 (4) - Selenium + Scrapy](https://heodolf.tistory.com/13?category=897877)
- [[크롤링] 무작정 시작하기 (7) - scrapyd](https://heodolf.tistory.com/28?category=897877)
- [Scrapy: How to loop a crawler](http://crawl.blog/scrapy-loop.html)